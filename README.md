# Bidirectional Human-AI Alignment - Reading List


This repository lists papers on **[Bidirectional Human-AI Alignment](https://arxiv.org/pdf/2406.09264)** [Paper](https://arxiv.org/pdf/2406.09264) research, primarily covering papers in Human-Computer Interaction (HCI), Natural Language Processing (NLP) and Machine Learng (ML) fields. 

<!-- Reference: https://github.com/zhijing-jin/Causality4NLP_Papers -->


**Contributor:** [Hua Shen](https://hua-shen.org/).
Welcome to be a collaborator, -- you can make an issue/pull request, and I can add you :).


### Contents (Actively Updating)





- [1. Alignment Basics](#1-basics)
  - [1.1 Overview Papers](#11-overview)
  - [1.2 Alignment Goals and Human Value Theories](#12-goals-values)

- [2. Human Values and Specifications](#1-human-values)
  - [2.1 Human Value Category](#11-human-value-category)
    - [2.1.1 Source of Values](#111-source)
    - [2.1.2 Value Types](#112-types)
  - [2.2 Interaction Techniques to Specify AI Values](#12-value-pecification)
    - [2.2.1 Explicit Human Feedback](#121-explicit-feedback)
    - [2.2.2 Implicit Human Feedback](#122-implicit-feedback)
    - [2.2.3 Simulated Human Value Feedback](#123-simulated-feedback)

  
- [3. Integrating Human Specifications into AI](#2-integrate-ai)
  - [3.1 Develop AI with General Values](#21-human-value-category)
    - [3.1.1 Instruction Data](#211-instruction-data)
    - [3.1.2 Model Learning](#212-model-learning)
    - [3.1.3 Inference Stage](#213-inference-stage)
  - [3.2 Customizing AI for Individuals and Groups](#22-customize-ai)
    - [3.2.1 Customized Data](#221-customize-data)
    - [3.2.2 Adapt Model by Learning](#222-adapt-model-by-learning)
    - [3.2.3 Interactive Alignment](#223-interactive-alignment)
  - [3.3 Evaluating AI Systems](#23-evaluate-ai)
    - [3.3.1 Human-In-The-Loop-Evaluation](#231-hitl-eval)
    - [3.3.2 Automatic Evaluation](#232-auto-eval)
  - [3.4 Ecosystem](#24-ecosystem)
    - [3.4.1 Platforms](#241-platforms)
    

- [4.Human Cognitive Adjustment to AI](#3-cognitive-adjustment)
  - [4.1 Perceiving and Understanding of AI](#31-perceive)
    - [4.1.1 Education and Training Human](#311-education-training)
    - [4.1.2 AI Sensemaking and Explanations](#312-ai-sensemaking)
  - [4.2 Critical Thinking about AI](#32-critical-thinking)
    - [4.2.1 Trust and Reliance on AI Decisions](#321-trust-reliance)
    - [4.2.2 Ethical Concerns and AI Auditing](#322-ethical-audit)
    - [4.2.3 Calibrate Cognition to Align AI](#323-calibrate-ai)


- [5. Human Adaptive Behavior to AI](#4-behavioral-adapation)
  - [5.1 Human Collaborating with Diverse AI Roles](#41-human-ai-collaboration)
    - [5.1.1 Assistants](#411-assistants)
    - [5.1.2 Partners](#412-partners)
    - [5.1.3 Tutors](#413-tutors)
  - [5.2 AI Impacts on Human and Society](#42-ai-impacts)
    - [5.2.1 Impact on Individual Behavior](#421-impact-on-individual)
    - [5.2.2 Societal Concerns and AI Impacts](#422-social-impacts)
    - [5.2.3 Reaction to AI Advancements](#423-reaction-to-ai)
  - [5.3 Evaluation in Human Studies](#43-evaluation)
    - [5.3.1 Evaluate Human-AI Collaboration](#431-evaluate-collaborate)
    - [5.3.2 Evaluate Societal Impact](#432-eval-social-impacts)


- [6. Others - To Add More...](#5-more)



## 1. Alignment Basics

### 1.1 Overview Papers (Chronological)

1. **Open problems and fundamental limitations of reinforcement learning from human feedback.** *Casper, Stephen, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman et al.*, Sep 2023. [[pdf]](https://arxiv.org/pdf/2307.15217)


2. **AI alignment in the design of interactive AI: Specification alignment, process alignment, and evaluation support.** *Terry, Michael, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, and Meredith Ringel Morris.*, Oct 2023. [[pdf]](https://arxiv.org/pdf/2311.00710)


3. **A roadmap to pluralistic alignment.** *Sorensen, Taylor, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye et al.*, Feb 2024. [[pdf]](https://arxiv.org/pdf/2402.05070)


4. **Foundational challenges in assuring alignment and safety of large language models.** *Anwar, Usman, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana et al.*, Apr 2024. [[pdf]](https://arxiv.org/pdf/2404.09932)


5. **Managing extreme AI risks amid rapid progress.** *Bengio, Yoshua, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari et al.*, May 2024. [[pdf]](https://arxiv.org/pdf/2310.17688)


6. **Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions.** *Shen, Hua, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma et al.*, Jun 2024. [[pdf]](https://arxiv.org/pdf/2406.09264)


### 1.2 Alignment Goals and Human Value Theories

1. **Artificial intelligence, values, and alignment.** *Gabriel, Iason.*, 2020. [[pdf]](https://link.springer.com/content/pdf/10.1007/s11023-020-09539-2.pdf)

2. (Schwartz Theory of Basic Values) **Are there universal aspects in the structure and contents of human values?.** *Schwartz, Shalom H.*, 1994. [[pdf]](https://spssi.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1540-4560.1994.tb01196.x)


3. (Schwartz Theory of Basic Values) **An overview of the Schwartz theory of basic values.** *Schwartz, Shalom H.*, 2012. [[pdf]](https://scholarworks.gvsu.edu/cgi/viewcontent.cgi?article=1116&context=orpc)

4. (Moral Foundation Theory) **Moral foundations theory: The pragmatic validity of moral pluralism.** *Graham, Jesse, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P. Wojcik, and Peter H. Ditto.*, 2013. [[pdf]](https://www.sciencedirect.com/science/article/pii/B9780124072367000024)




## 2. Human Values and Specifications

### 2.1 Human Value Category

#### 2.1.1 Source of Values

**Individuals:**


1. (2019 CHI) **Improving fairness in machine learning systems: What do industry practitioners need?.** *Holstein, Kenneth, Jennifer Wortman Vaughan, Hal Daumé III, Miro Dudik, and Hanna Wallach.* [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3290605.3300830)

3. (2022 Arxiv) **Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.** *Ganguli, Deep, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann et al.*  arXiv preprint arXiv:2209.07858 (2022).

2. (2023 EMNLP) **Ethical reasoning over moral alignment: A case and framework for in-context ethical policies in LLMs.** *Rao, Abhinav, Aditi Khandelwal, Kumar Tanmay, Utkarsh Agarwal, and Monojit Choudhury.* [[pdf]](https://aclanthology.org/2023.findings-emnlp.892.pdf)



3. (2021 CHI) **Human perceptions on moral responsibility of AI: A case study in AI-assisted bail decision-making**. *Lima, Gabriel, Nina Grgić-Hlača, and Meeyoung Cha*. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3411764.3445260?casa_token=1f8C9397M5UAAAAA:axhwwncFHhzsQgRueN8OER4kJuAGVO61zL7ic3OJviG-8FYuPY3dbdzh77M05Ko0xXi3RhU6h9T4)
<!-- {:target="_blank"} -->


**Society:**


1. (2023 EMNLP) **Do llms understand social knowledge? evaluating the sociability of large language models with socket benchmark.** *Choi, Minje, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens.* [[pdf]](https://arxiv.org/pdf/2305.14938)

2. (2023 CHI) **Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making.** *Zheng, Chengbo, Yuheng Wu, Chuhan Shi, Shuai Ma, Jiehui Luo, and Xiaojuan Ma.*

2. (2023 ACL) **The ecological fallacy in annotation: Modelling human label variation goes beyond sociodemographics.** *Orlikowski, Matthias, Paul Röttger, Philipp Cimiano, and Dirk Hovy.* [[pdf]](https://aclanthology.org/2023.acl-short.88.pdf)


1. (2023 Arxiv) **TASRA: a taxonomy and analysis of societal-scale risks from AI.** *Critch, Andrew, and Stuart Russell.* arXiv preprint arXiv:2306.06924 (2023). [[pdf]](https://arxiv.org/pdf/2306.06924)


5. (2020 EMNLP) **Social chemistry 101: Learning to reason about social and moral norms.** *Forbes, Maxwell, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi.*  [[pdf]](https://arxiv.org/pdf/2011.00620)


**Interaction:**


1. (2023 EMNLP) **FANToM: A benchmark for stress-testing machine theory of mind in interactions.** *Kim, Hyunwoo, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap.* [[pdf]](https://aclanthology.org/2023.emnlp-main.890.pdf)


2. (2024 NeurIPS) **Lima: Less is more for alignment.** *Zhou, Chunting, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma et al.* [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2023/file/ac662d74829e4407ce1d126477f4a03a-Paper-Conference.pdf)




#### 2.1.2 Value Types


**Self-Enhancement:**

<!-- 1. (2024 CHI) **Rehearsal: Simulating conflict to teach conflict resolution.** *Shaikh, Omar, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael S. Bernstein.* [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3613904.3642159)


2. (2024 AIED) **How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging.** *Ma, Qianou, Hua Shen, Kenneth Koedinger, and Sherry Tongshuang Wu.* [[pdf]](https://www.cs.cmu.edu/~sherryw/assets/pubs/2024-hypocompass.pdf) -->


**Self-Transcendence:**

1. (2023 NeurIPS) **Auditing for human expertise.** *Alur, Rohan, Loren Laine, Darrick Li, Manish Raghavan, Devavrat Shah, and Dennis Shung.* [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2023/file/fb44a668c2d4bc984e9d6ca261262cbb-Paper-Conference.pdf)


2. (2022 Arxiv) **Training a helpful and harmless assistant with reinforcement learning from human feedback.** *Bai, Yuntao, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain et al.* [[pdf]](https://arxiv.org/pdf/2204.05862?spm=a2c6h.13046898.publish-article.36.6cd56ffaIPu4NQ&file=2204.05862)

**Conservation:**

1. (2022 Arxiv) **Constitutional ai: Harmlessness from ai feedback.** *Bai, Yuntao, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen et al.* [[pdf]](https://arxiv.org/pdf/2212.08073)

2. (2024, ICLR) **Fine-tuning aligned language models compromises safety, even when users do not intend to!.** *Qi, Xiangyu, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.* [[pdf]](https://openreview.net/pdf?id=hTEGyKf0dZ)


**Openness to Change:**

1. (2023 ACL) **Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions.** *Chung, John Joon Young, Ece Kamar, and Saleema Amershi.* [[pdf]](https://aclanthology.org/2023.acl-long.34.pdf)

2. (2024 ICLR) **Fine-tuning language models for factuality.** *Tian, Katherine, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn.* [[pdf]](https://arxiv.org/pdf/2311.08401)


**Desired Values from AI Tools:**

1. (2024 ICLR) **Generative judge for evaluating alignment.** *Li, Junlong, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu.* [[pdf]](https://arxiv.org/pdf/2310.05470)

2. (2022 ACL) **Are shortest rationales the best explanations for human understanding?.** *Shen, Hua, Tongshuang Wu, Wenbo Guo, and Ting-Hao'Kenneth Huang.* [[pdf]](https://arxiv.org/pdf/2203.08788)


### 2.2 Interaction Techniques to Specify AI Values

#### 2.2.1 Explicit Human Feedback

1. (2022 NeurIPS) **Training language models to follow instructions with human feedback.** *Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang et al.*  [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

2. (2023 ACL) **Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs.** Akyürek, Afra Feyza, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket Tandon. [[pdf]](https://arxiv.org/pdf/2305.08844)

3. (2023 IUI) **Constitutionmaker: Interactively critiquing large language models by converting feedback into principles.** *Petridis, Savvas, Benjamin D. Wedin, James Wexler, Mahima Pushkarna, Aaron Donsbach, Nitesh Goyal, Carrie J. Cai, and Michael Terry.* [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3640543.3645144)


#### 2.2.2 Implicit Human Feedback

1. (2023 EMNLP) **Towards a holistic landscape of situated theory of mind in large language models.** *Ma, Ziqiao, Jacob Sansom, Run Peng, and Joyce Chai.*  [[pdf]](https://aclanthology.org/2023.findings-emnlp.72.pdf)

2. (2023 EMNLP) **Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models.** *He, Yinghui, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng.* [[pdf]](https://arxiv.org/pdf/2310.16755)

3. (2021 EMNLP) **A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational ai systems.** *Park, Sunghyun, Han Li, Ameen Patel, Sidharth Mudgal, Sungjin Lee, Young-Bum Kim, Spyros Matsoukas, and Ruhi Sarikaya.* [[pdf]](https://aclanthology.org/2021.emnlp-main.489.pdf)


#### 2.2.3 Simulated Human Value Feedback

1. (2024 ICLR) **Identifying the risks of lm agents with an lm-emulated sandbox.** *Ruan, Yangjun, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J. Maddison, and Tatsunori Hashimoto.* [[pdf]](https://openreview.net/pdf?id=GEcwtMk1uA)

2. (2023 EMNLP) **Aligning large language models through synthetic feedback.** *Kim, Sungdong, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon Seo.* [[pdf]](https://aclanthology.org/2023.emnlp-main.844.pdf)


## 3. Integrating Human Specifications into AI

### 3.1 Develop AI with General Values

#### 3.1.1 Instruction Data

1. Shen, Hua, Vicky Zayats, Johann C. Rocholl, Daniel D. Walker, and Dirk Padfield. "Multiturncleanup: A benchmark for multi-turn spoken conversational transcript cleanup." arXiv preprint arXiv:2305.12029 (2023). [[pdf]](https://arxiv.org/pdf/2305.12029)


2. Li, Minzhi, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F. Chen, Zhengyuan Liu, and Diyi Yang. "Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation." arXiv preprint arXiv:2310.15638 (2023). [[pdf]](https://aclanthology.org/2023.emnlp-main.92.pdf)

3. Yuan, Hongyi, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. "RRHF: Rank responses to align language models with human feedback." Advances in Neural Information Processing Systems 36 (2024). [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2023/file/23e6f78bdec844a9f7b6c957de2aae91-Paper-Conference.pdf)

#### 3.1.2 Model Learning

1. Rafailov, Rafael, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. "Direct preference optimization: Your language model is secretly a reward model." Advances in Neural Information Processing Systems 36 (2024). [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf)


2. (2023 TMLR) Dong, Hanze, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. "Raft: Reward ranked finetuning for generative foundation model alignment." arXiv preprint arXiv:2304.06767 (2023). [[pdf]](https://arxiv.org/pdf/2304.06767)

#### 3.1.3 Inference Stage

1. (2023 EMNLP) Wang, Zhilin, Yu Ying Chiu, and Yu Cheung Chiu. "Humanoid agents: Platform for simulating human-like generative agents." arXiv preprint arXiv:2310.05418 (2023). [[pdf]](https://arxiv.org/pdf/2310.05418)


2. (2023 ICLR) Gou, Zhibin, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. "Critic: Large language models can self-correct with tool-interactive critiquing." arXiv preprint arXiv:2305.11738 (2023). [[pdf]](https://arxiv.org/pdf/2305.11738)

3. (2024 ICLR) **Quality-diversity through ai feedback**. *Bradley, Herbie, Andrew Dai, Hannah Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, Jeff Clune, Kenneth Stanley, Grégory Schott, and Joel Lehman.* [[pdf]](https://openreview.net/pdf?id=owokKCrGYr)


### 3.2 Customizing AI for Individuals and Groups

#### 3.2.1 Customized Data

1. (2023 ACL) Felkner, Virginia K., Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. "Winoqueer: A community-in-the-loop benchmark for anti-lgbtq+ bias in large language models." arXiv preprint arXiv:2306.15087 (2023). [[pdf]](https://arxiv.org/pdf/2306.15087)

#### 3.2.2 Adapt Model by Learning

1. (2024 ICLR) Zhao, Siyan, John Dang, and Aditya Grover. "Group preference optimization: Few-shot alignment of large language models." arXiv preprint arXiv:2310.11523 (2023). [[pdf]](https://openreview.net/forum?id=DpFeMH4l8Q)

2. (2022 AAMAS) Peschl, Markus, Arkady Zgonnikov, Frans A. Oliehoek, and Luciano C. Siebert. "MORAL: Aligning AI with human norms through multi-objective reinforced active learning." arXiv preprint arXiv:2201.00012 (2021). [[pdf]](https://arxiv.org/pdf/2201.00012)


3. (2022 EMNLP) Sun, Tianxiang, Junliang He, Xipeng Qiu, and Xuanjing Huang. "BERTScore is unfair: On social bias in language model-based metrics for text generation." arXiv preprint arXiv:2210.07626 (2022). [[pdf]](https://arxiv.org/pdf/2210.07626)


4. (2022 ICASSP) Shen, Hua, Yuguang Yang, Guoli Sun, Ryan Langman, Eunjung Han, Jasha Droppo, and Andreas Stolcke. "Improving fairness in speaker verification via group-adapted fusion network." In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7077-7081. IEEE, 2022. [[pdf]](https://arxiv.org/pdf/2202.11323)

5. (2023 EMNLP) Wang, Xingjin, Linjing Li, and Daniel Zeng. "LDM $^ 2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement." arXiv preprint arXiv:2312.08402 (2023). [[pdf]](https://arxiv.org/pdf/2312.08402)



#### 3.2.3 Interactive Alignment

1. (2023 ICLR) Maghakian, Jessica, Paul Mineiro, Kishan Panaganti, Mark Rucker, Akanksha Saran, and Cheng Tan. "Personalized reward learning with interaction-grounded learning (IGL)." arXiv preprint arXiv:2211.15823 (2022). [[pdf]](https://arxiv.org/pdf/2211.15823)

2. (2023 EMNLP) Lahoti, Preethi, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan et al. "Improving diversity of demographic representation in large language models via collective-critiques and self-voting." arXiv preprint arXiv:2310.16523 (2023). [[pdf]](https://arxiv.org/pdf/2310.16523)

3. (2022 ACL) Welch, Charles, Chenxi Gu, Jonathan K. Kummerfeld, Verónica Pérez-Rosas, and Rada Mihalcea. "Leveraging similar users for personalized language modeling with limited data." In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1742-1752. 2022. [[pdf]](https://aclanthology.org/2022.acl-long.122.pdf)


### 3.3 Evaluating AI Systems

#### 3.3.1 Human-In-The-Loop-Evaluation

1. (2022 NAACL) Liu, Ruibo, Ge Zhang, Xinyu Feng, and Soroush Vosoughi. "Aligning generative language models with human values." In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 241-252. 2022. [[pdf]](https://aclanthology.org/2022.findings-naacl.18.pdf)


2. (2023 ACL) Kwon, Bum Chul, and Nandana Mihindukulasooriya. "Finspector: A human-centered visual inspection tool for exploring and comparing biases among foundation models." arXiv preprint arXiv:2305.16937 (2023). [[pdf]](https://aclanthology.org/2023.acl-demo.4.pdf)


#### 3.3.2 Automatic Evaluation

1. (2024 NeurIPS) Sun, Zhiqing, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. "Principle-driven self-alignment of language models from scratch with minimal human supervision." Advances in Neural Information Processing Systems 36 (2024). [[pdf]](https://proceedings.neurips.cc/paper_files/paper/2023/file/0764db1151b936aca59249e2c1386101-Paper-Conference.pdf)


2. (2023 EMNLP) Lee, Noah, Na Min An, and James Thorne. "Can Large Language Models Capture Dissenting Human Voices?." arXiv preprint arXiv:2305.13788 (2023). [[pdf]](https://arxiv.org/pdf/2305.13788)



### 3.4 Ecosystem

#### 3.4.1 Platforms

1. (2023 NeurIPS) *Dubois, Yann, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S. Liang, and Tatsunori B. Hashimoto.* **Alpacafarm: A simulation framework for methods that learn from human feedback.** [[pdf]](https://openreview.net/pdf?id=4hturzLcKX)


2. (2023 EMNLP) Xu, Binfeng, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, Ziyu Yao, and Dongkuan Xu. "Gentopia: A collaborative platform for tool-augmented llms." arXiv preprint arXiv:2308.04030 (2023). [[pdf]](https://arxiv.org/pdf/2308.04030)


3. (2022 EMNLP) Pei, Jiaxin, Aparna Ananthasubramaniam, Xingyao Wang, Naitian Zhou, Jackson Sargent, Apostolos Dedeloudis, and David Jurgens. "Potato: The portable text annotation tool." arXiv preprint arXiv:2212.08620 (2022). [[pdf]](https://aclanthology.org/2022.emnlp-demos.33.pdf)


4. (2024 ICLR) Yuan, Yifu, Jianye Hao, Yi Ma, Zibin Dong, Hebin Liang, Jinyi Liu, Zhixin Feng, Kai Zhao, and Yan Zheng. "Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback." arXiv preprint arXiv:2402.02423 (2024). [[pdf]](https://arxiv.org/pdf/2402.02423)




## 4.Human Cognitive Adjustment to AI

### 4.1 Perceiving and Understanding of AI


#### 4.1.1 Education and Training Human

1. (2022 CHI) Long, Duri, and Brian Magerko. "What is AI literacy? Competencies and design considerations." In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 1-16. 2020. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3313831.3376727?casa_token=MWvd1NHw0HEAAAAA:xtkNw2v4XHT_wofSw4W6mvuGoN8wVH7536AkVVNLwwqTVB7ANk4J7UM-BA-B4JUCT8pT2qywJXpR)


2. (2020 CSCW) McDonald, Nora, and Shimei Pan. "Intersectional AI: A study of how information science students think about ethics and their impact." Proceedings of the ACM on Human-Computer Interaction 4, no. CSCW2 (2020): 1-19. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3415218?casa_token=kPImRsLtrCYAAAAA:YW_O_1MPsXHeJc0Tmi7TA8iu3dQaGaS_n86DAJc_0jSHBTWI6AbDqGwgNLeQ_VXbpCIz19RmZmMu)


#### 4.1.2 AI Sensemaking and Explanations

1. (2023 CHI) Petridis, Savvas, Nicholas Diakopoulos, Kevin Crowston, Mark Hansen, Keren Henderson, Stan Jastrzebski, Jeffrey V. Nickerson, and Lydia B. Chilton. "Anglekindling: Supporting journalistic angle ideation with large language models." In Proceedings of the 2023 CHI conference on human factors in computing systems, pp. 1-16. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3544548.3580907?casa_token=G5q-owxcuakAAAAA:awS7rEcVc6pJ9GofC6azdkXEc5aI6MNcWdTdlcwKdoOGsI5aZzJAepEMT6FIS2jFERiconlMWe3N)

2. (2020 EMNLP) Tenney, Ian, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang et al. "The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models." arXiv preprint arXiv:2008.05122 (2020). [[pdf]](https://aclanthology.org/2020.emnlp-demos.15.pdf)

3. (2023 CSCW Demo) Shen, Hua, Chieh-Yang Huang, Tongshuang Wu, and Ting-Hao Kenneth Huang. "ConvXAI: Delivering heterogeneous AI explanations via conversations to support human-AI scientific writing." In Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing, pp. 384-387. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3584931.3607492?casa_token=TEUv5IwwP9cAAAAA:9iQ9rwkkF7ShMYEHYsGxHGxZfVye-da5M6fwLF5GSSIsmswpdvd6F9JO2Rch0QMNBwvyqLKjKIMq)



### 4.2 Critical Thinking about AI

#### 4.2.1 Trust and Reliance on AI Decisions

1. (2019 CHI) Yin, Ming, Jennifer Wortman Vaughan, and Hanna Wallach. "Understanding the effect of accuracy on trust in machine learning models." In Proceedings of the 2019 chi conference on human factors in computing systems, pp. 1-12. 2019. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3290605.3300509?casa_token=OTKgPNLNKa0AAAAA:2d3J1UX48ji_-K-F5Maz-PHj_Ap5Yb2bLF6BBxJEgZdA6Cr1eAARJfRZd3eBO924QbLVSayW0QBQ)

2. (2023 IUI) Schemmer, Max, Niklas Kuehl, Carina Benz, Andrea Bartos, and Gerhard Satzger. "Appropriate reliance on AI advice: Conceptualization and the effect of explanations." In Proceedings of the 28th International Conference on Intelligent User Interfaces, pp. 410-422. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3581641.3584066?casa_token=M315CFR5-BwAAAAA:HQNa200ItS9rIkGSYb1ARq1XYAbO1hXXskk2QoBwgB-XLilo7A8Nh5NFr2AOJFE26aomZP9ivNyN)


3. (2024 CHI) Gu, Ziwei, Ian Arawjo, Kenneth Li, Jonathan K. Kummerfeld, and Elena L. Glassman. "An AI-Resilient Text Rendering Technique for Reading and Skimming Documents." In Proceedings of the CHI Conference on Human Factors in Computing Systems, pp. 1-22. 2024. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3613904.3642699?casa_token=NtY359Zv7JgAAAAA:SR-Y5kOnKe_3vfCc3i7Cg3oEpzI2pa4zhq8xnr0ASOLPP_tOyXmt7bzWT1NagW7pU350MvlxK591)



#### 4.2.2 Ethical Concerns and AI Auditing

1. (2020 CHI) Madaio, Michael A., Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. "Co-designing checklists to understand organizational challenges and opportunities around fairness in AI." In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 1-14. 2020. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3313831.3376445)

2. (2021 CSCW) **Problematic machine behavior: A systematic literature review of algorithm audits.** *Bandy, Jack.* [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3449148)



#### 4.2.3 Calibrate Cognition to Align AI

1. (2019 CHI) Kocielnik, Rafal, Saleema Amershi, and Paul N. Bennett. "Will you accept an imperfect ai? exploring designs for adjusting end-user expectations of ai systems." In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1-14. 2019. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3290605.3300641?casa_token=OLB1htBBaNkAAAAA:UniqTgSTY3ruCB_AFKMRye9MZcsILJx6f3w0qAfRA8Z_uVxHzvmzfxAoyNwukN1kwhBSw7xWp7PL)

2. (2023 CHI) Wischnewski, Magdalena, Nicole Krämer, and Emmanuel Müller. "Measuring and understanding trust calibrations for automated systems: a survey of the state-of-the-art and future directions." In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pp. 1-16. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3544548.3581197)


## 5. Human Adaptive Behavior to AI

### 5.1 Human Collaborating with Diverse AI Roles

#### 5.1.1 Assistants

1. (2022 CHI) Wu, Tongshuang, Michael Terry, and Carrie Jun Cai. "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts." In Proceedings of the 2022 CHI conference on human factors in computing systems, pp. 1-22. 2022. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3491102.3517582)


2. (2023 CHI) Gebreegziabher, Simret Araya, Zheng Zhang, Xiaohang Tang, Yihao Meng, Elena L. Glassman, and Toby Jia-Jun Li. "Patat: Human-ai collaborative qualitative coding with explainable interactive rule synthesis." In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pp. 1-19. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3544548.3581352?casa_token=u1rVwBlK6tsAAAAA:3-9xZ34ortz9P-JPWXBIBviwlG7au4IfhBYM3iJyFlJQW6CFzlPH1hh17YNVC_KGurMoYGemy9VV)


3. (2019 CHI) Choi, Minsuk, Cheonbok Park, Soyoung Yang, Yonggyu Kim, Jaegul Choo, and Sungsoo Ray Hong. "Aila: Attentive interactive labeling assistant for document classification through attention-based deep neural networks." In Proceedings of the 2019 CHI conference on human factors in computing systems, pp. 1-12. 2019. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3290605.3300460?casa_token=aGfb-S-YMhEAAAAA:IWMBGBUvx0jAYpfUfWN6ObXUlpoHD9JrRsxZ7OvIfn1UEifvOWmWJM7YUbx8BE9O0XOP5al6h2pS)




#### 5.1.2 Partners

1. (2023 UIST) Park, Joon Sung, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. "Generative agents: Interactive simulacra of human behavior." In Proceedings of the 36th annual acm symposium on user interface software and technology, pp. 1-22. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3586183.3606763)


2. Zagalsky, Alexey, Dov Te'eni, Inbal Yahav, David G. Schwartz, Gahl Silverman, Daniel Cohen, Yossi Mann, and Dafna Lewinsky. "The design of reciprocal learning between human and artificial intelligence." Proceedings of the ACM on Human-Computer Interaction 5, no. CSCW2 (2021): 1-36. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3479587?casa_token=YFcCEFP8bZ4AAAAA:xEPS4jm8HVqD2GeDZMhi5WeIRUaAIY9RQiOYuiwx35P3YSEc4a55UOQhI3uKqeRc4IqAOkW5pNfy)


3. (2023 CHI) Pinski, Marc, Martin Adam, and Alexander Benlian. "AI knowledge: Improving AI delegation through human enablement." In Proceedings of the 2023 CHI conference on human factors in computing systems, pp. 1-17. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3544548.3580794?casa_token=y-0Fq9LMzGoAAAAA:F_-vQ8wBm8O0Kk3483UPFprKStj9obQQF5lvsWbnG0c_YiztTzw8-LK9M7Tfd7SmKAvqtJCk3lNi)


4. Sundar, S. Shyam, and Eun-Ju Lee. "Rethinking communication in the era of artificial intelligence." Human Communication Research 48, no. 3 (2022): 379-385. [[pdf]](https://academic.oup.com/hcr/article-abstract/48/3/379/6620825?redirectedFrom=PDF&casa_token=WsZy3Xfn3_MAAAAA:_UQiLadxs5ABVFR3uB9EtuOy4l-qihiqsASITi2nxIXPSYNUMZOhNAm7m0w5JcjPA4GU9KICkGUGZQ)


5. Yildirim, Nur, Alex Kass, Teresa Tung, Connor Upton, Donnacha Costello, Robert Giusti, Sinem Lacin et al. "How experienced designers of enterprise applications engage AI as a design material." In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pp. 1-13. 2022. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3491102.3517491)


#### 5.1.3 Tutors

1. (2024 AIED) **How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging.** *Ma, Qianou, Hua Shen, Kenneth Koedinger, and Sherry Tongshuang Wu.* [[pdf]](https://www.cs.cmu.edu/~sherryw/assets/pubs/2024-hypocompass.pdf)


2. (2024 CHI) **Rehearsal: Simulating conflict to teach conflict resolution.** *Shaikh, Omar, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael S. Bernstein.* [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3613904.3642159)



### 5.2 AI Impacts on Human and Society

#### 5.2.1 Impact on Individual Behavior

1. Shen, Hua, and Ting-Hao Huang. "How useful are the machine-generated interpretations to general users? a human evaluation on guessing the incorrectly predicted labels." In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, vol. 8, pp. 168-172. 2020. [[pdf]](https://ojs.aaai.org/index.php/HCOMP/article/view/7477/7256)


2. Ashkinaze, Joshua, Julia Mendelsohn, Li Qiwei, Ceren Budak, and Eric Gilbert. "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment." arXiv preprint arXiv:2401.13481 (2024). [[pdf]](https://arxiv.org/pdf/2401.13481)



3. Gajos, Krzysztof Z., and Lena Mamykina. "Do people engage cognitively with AI? Impact of AI assistance on incidental learning." In Proceedings of the 27th International Conference on Intelligent User Interfaces, pp. 794-806. 2022. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3490099.3511138)


<!-- 3. Li, Tianshi, Sauvik Das, Hao-Ping Lee, Dakuo Wang, Bingsheng Yao, and Zhiping Zhang. "Human-Centered Privacy Research in the Age of Large Language Models." In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pp. 1-4. 2024. [[pdf]]() -->




#### 5.2.2 Societal Concerns and AI Impacts

1. Atreja, Shubham, Libby Hemphill, and Paul Resnick. "Remove, reduce, inform: what actions do people want Social Media platforms to take on potentially misleading content?." Proceedings of the ACM on Human-Computer Interaction 7, no. CSCW2 (2023): 1-33. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3610082?casa_token=kcoT4SDky3QAAAAA:ChztFk1kxFBgnAciq1oJLi-Te9OTZnlKZFMnMfto6R3-rcIZj3AR3apM9sssMmHQbn7EbQ3yx8gO)

2. (2023 ICLR) Kenny, Eoin M., Mycal Tucker, and Julie Shah. "Towards interpretable deep reinforcement learning with human-friendly prototypes." In The Eleventh International Conference on Learning Representations. 2023. [[pdf]](https://openreview.net/pdf?id=hWwY_Jq0xsN)

3. Cheng, Xusen, Xiaoping Zhang, Jason Cohen, and Jian Mou. "Human vs. AI: Understanding the impact of anthropomorphism on consumer response to chatbots from the perspective of trust and relationship norms." Information Processing & Management 59, no. 3 (2022): 102940. [[pdf]](https://www.sciencedirect.com/science/article/pii/S0306457322000620?casa_token=R_GDKLpW1KwAAAAA:vjF-C8qKfA5EyeWowZ9N43EA1XEpQlFryMQ_3bpTuvuWIJrQx8F-fBuepIgQxYhGgWgntfvQ)


4. Park, Hyanghee, Daehwan Ahn, Kartik Hosanagar, and Joonhwan Lee. "Human-AI interaction in human resource management: Understanding why employees resist algorithmic evaluation at workplaces and how to mitigate burdens." In Proceedings of the 2021 CHI conference on human factors in computing systems, pp. 1-15. 2021. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3411764.3445304?casa_token=UGBhg1Z31nkAAAAA:AotPhsLc-w4LiubqgV5owW-TFs5S6w7oAQJaS5svJyVwlz9Kc8uAQpFMIo5tvWsQ6_qBqK7JEJpm)



#### 5.2.3 Reaction to AI Advancements

1. Hacker, Philipp, Andreas Engel, and Marco Mauer. "Regulating ChatGPT and other large generative AI models." In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pp. 1112-1123. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3593013.3594067)


2. Lima, Gabriel, Nina Grgic-Hlaca, Jin Keun Jeong, and Meeyoung Cha. "Who Should Pay When Machines Cause Harm? Laypeople’s Expectations of Legal Damages for Machine-Caused Harm." In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pp. 236-246. 2023. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3593013.3593992)


3. (2021 CHI) Park, Hyanghee, Daehwan Ahn, Kartik Hosanagar, and Joonhwan Lee. "Human-AI interaction in human resource management: Understanding why employees resist algorithmic evaluation at workplaces and how to mitigate burdens." In Proceedings of the 2021 CHI conference on human factors in computing systems, pp. 1-15. 2021. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3411764.3445304?casa_token=sg-aQUgoRBsAAAAA:tk6rNjv_8qude-IhhZrieLhmBHBig64imKvpGgZq_tMpZq_l9mXNhgTOmpGdRKWiCuRMbzzmVtly)


4. Madaio, Michael A., Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. "Co-designing checklists to understand organizational challenges and opportunities around fairness in AI." In Proceedings of the 2020 CHI conference on human factors in computing systems, pp. 1-14. 2020. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3313831.3376445)


### 5.3 Evaluation in Human Studies

#### 5.3.1 Evaluate Human-AI Collaboration


1. Mozannar, Hussein, Valerie Chen, Mohammed Alsobay, Subhro Das, Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet Talwalkar, and David Sontag. "The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers." arXiv preprint arXiv:2404.02806 (2024). [[pdf]](https://arxiv.org/pdf/2404.02806)

2. Ding, Zijian, Alison Smith-Renner, Wenjuan Zhang, Joel R. Tetreault, and Alejandro Jaimes. "Harnessing the power of LLMs: Evaluating human-AI text co-creation through the lens of news headline generation." arXiv preprint arXiv:2310.10706 (2023). [[pdf]](https://arxiv.org/pdf/2310.10706)


3. (2023 CCS) Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. 2023. Do users write more insecure code with AI assistants? (2023), 2785–2799. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3576915.3623157?casa_token=DOFKsd6BLDcAAAAA:nz3SyrTaFV3igBRRz-ftObi_gQT-VrARkwzTSCLtyuPzOA6U6UHm699v2fGuLLh-fsLCZcofnPvf)


4. Zhang, Angie, Olympia Walker, Kaci Nguyen, Jiajun Dai, Anqing Chen, and Min Kyung Lee. "Deliberating with AI: improving decision-making for the future through participatory AI design and stakeholder deliberation." Proceedings of the ACM on Human-Computer Interaction 7, no. CSCW1 (2023): 1-32. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3579601)



5. Wang, Qiaosi, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. "Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant." In Proceedings of the 2021 CHI conference on human factors in computing systems, pp. 1-14. 2021. [[pdf]](https://dl.acm.org/doi/pdf/10.1145/3411764.3445645?casa_token=HbHpIfbqCBcAAAAA:pO8IredpTSBqhL8EBYzHS3MmM3Q_qKdEyBlyTmEXD04xbESNtPfbQexWpUtKorzd6Cvg_LgIm9rb)




#### 5.3.2 Evaluate Societal Impact


1. Santurkar, Shibani, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. "Whose opinions do language models reflect?." In International Conference on Machine Learning, pp. 29971-30004. PMLR, 2023. [[pdf]](https://proceedings.mlr.press/v202/santurkar23a/santurkar23a.pdf)


2. (2024 PNAS) Zhou, Eric, and Dokyun Lee. "Generative artificial intelligence, human creativity, and art." PNAS nexus 3, no. 3 (2024): pgae052. [[pdf]](https://watermark.silverchair.com/pgae052.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA18wggNbBgkqhkiG9w0BBwagggNMMIIDSAIBADCCA0EGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMmh4z8Te_EeV3HkiMAgEQgIIDEmvEvC_tvnFV5oo8_ZvmgKDMTfc9gZClw4LT9wc2z4D1OgsqBoHawPCLpd700Ph9REZ4pJsHflb-7flavdxU7OxBwKIP8GbCkGW6JKSG43AAXdPe-ZwOmrMwKwGLx4fiaaS7v4hZZOs69ELMNXX0imoF2ejV-OUg1tmjLsba463Wnf9SZPkO7sKzTLpx6sVSaRU4p_gjtkHqotwFkF2SV15W2da84W9-oWSmXWSpqMC6TGAzmlZ6sqcs4FXha2kW_GjHv3iTwEiHy_asJ0Pd1fp06qZBoIzt_wLoex_7XY5Fh1qfv8tC1lyWWjVx6rd8GhcL2lAjXkTx7LP1zvIWueI3Mjuv_suuA2D9mfQjvZ0FHiV_R7RG0JS_vHuZ93kG73Hq-zjdDpf-396YbxvtOxwR5pWH5JQWli8LXEFBYLLvZ5XwZTxQYFz3Sb9soNfLC2r6QzG68CGzVvy7gMeqN_OegL3_89rU-CVyexOX4nhE8fx_QMUom_3d0asSTpibGWj1GjQCaxInTRMzrbOflObXqbco6hBb9-fRXDyzGy8Tjy_Uh1Ruz6d9qB66SEnu0THYCwgXNz5U8VsXS4rfBOBwEjcdxhjYDca42Ikq9VJvzDk43EyVutSWh8ucHnUUZr4OO01hlnC50E2BOmqwQFb9C9r2GNORVg2K24zLsVmImRq4syuDnlBA5GsQeI65KLGIUnQ18pOgMyZdmK29MEByr9g2N8TpztLCREDq4Bbv8PRXZS9SKVRD9K9iRY1N5CheW-WRXAcJVgQueoBMK1ysstJlSZjEi5AuozeroOBGud5wRIB8y2iy92i_RPekF3GmgSsU_t1M2DcAHEXLSY0GPBGAJJvziPw-v2qmrLoeKNRgTkat5uuB8jDB8xa_zIv4LcjzwKVLduNJph2f9Y04SBJ0VHWnuVCa4odRtNhvYoIys4uodYAbASVHuqXxakTufNcv4LALjKiGFjFjqKFbJJKkOQLIOdW2E3MvOebmjVM1QJilx_1dUXCuB4pQHqNYNZtMYC3RHkpkLWbb9bkZmQ)







<!-- ## 1. Causality Basics

### 1.3 Toolboxes

#### Causal Discovery

1. (2021) **causal-learn (Python package for causal discovery).** _Carnegie Mellon University_. [[GitHub](https://github.com/cmu-phil/causal-learn)] [[documentation](https://causal-learn.readthedocs.io/en/latest/)] 
3. (2019) **Causal Discovery Toolbox in Python.** [[GitHub](https://github.com/FenTechSolutions/CausalDiscoveryToolbox)] [[pdf](https://arxiv.org/pdf/1903.02278.pdf)] 
4. **Causal discovery tools.** _University of Pittsburgh/Carnegie Mellon University Center for Causal Discovery_. [[link](https://www.ccd.pitt.edu/tools/)]
   <br>e.g., Tretrad, [py-causal](https://bd2kccd.github.io/docs/py-causal/)  -->








## How to Cite This Repo

If you find the repository helpful to your research and would like to cite it, please see the `bibtex` below:

```bibtex
@article{shen2024towards,
  title={Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions},
  author={Shen, Hua and Knearem, Tiffany and Ghosh, Reshmi and Alkiek, Kenan and Krishna, Kundan and Liu, Yachuan and Ma, Ziqiao and Petridis, Savvas and Peng, Yi-Hao and Qiwei, Li and Rakshit, Sushrita and Si, Chenglei and Xie, Yutong and Bigham, Jeffrey P. and Bentley, Frank and Chai, Joyce and Lipton, Zachary and Mei, Qiaozhu and Mihalcea, Rada and Terry, Michael and Yang, Diyi and Morris, Meredith Ringel and Resnick, Paul and Jurgens, David},
  journal={arXiv preprint arXiv:2406.09264},
  year={2024}
}
```






<!-- 
- [1. Causality Basics](#1-causality-basics)
  - [1.1 Talks/Tutorial/etc](#11-Talkstutorialetc)
  - [1.2 Overview Papers](#12-overview-papers)
  - [1.3 Toolboxes](#13-toolboxes)
   -->


<!-- 
- [1. Causality Basics](#1-causality-basics)
  - [1.1 Talks/Tutorial/etc](#11-Talkstutorialetc)
  - [1.2 Overview Papers](#12-overview-papers)
  - [1.3 Toolboxes](#13-toolboxes)
- [2. Causality Applied to General NLP](#2-causality-applied-to-general-nlp)
  - [2.1 Causality to Bring Insights to NLP Modeling (for Robustness, Domain Adaptation, etc)](#21-causality-to-bring-insights-to-nlp-modeling-for-robustness-domain-adaptation-etc)
  - [2.2 Language Model Analysis in a Causal Way (for Probing, Interpretability, etc.)](#22-language-model-analysis-in-a-causal-way-for-probing-interpretability-etc)
  - [2.3 Text Features in Causal Graphs (for Social Science, Psychology, etc.)](#23-text-features-in-causal-graphs-for-social-science-psychology-etc)
  - [2.4 Causal Relation Extraction](#24-causal-relation-extraction)
  - [2.5 Causal Commonsense Reasoning and Generation](#25-causal-commonsense-reasoning-and-generation)
- [3. Causality for Various Applications](#3-causality-for-various-applications)
  - [3.1 Persuasion](#31-persuation)
  - [3.2 Psychology and Behavior](#32-psychology-and-behavior)
  - [3.3 Economics](#33-economics)
  - [3.4 Healthcare](#34-healthcare)
  - [3.4 Judicial Decision](#35-judicial-decision)
  - [3.5 Marketing strategies and sales prediction](#36-marketing-strategies-and-sales-prediction)
- [4. More Resources](#4-more-resources)
  - [4.1 Causality Papers from Schoelkopf's Lab, MPI](#41-causality-papers-from-schoelkopfs-lab-mpi)
    - [4.1.0 Overview](#410-overview)
    - [4.1.1 Learning Causal "Units" and Mechanisms (i.e., Causal Representation Learning)](#411-learning-causal-units-and-mechanisms-ie-causal-representation-learning)
    - [4.1.2 Robustness and Invariance (incl. Semi-Supervised Learning, Covariate Shift, Transfer Learning)](#411-learning-causal-units-and-mechanisms-ie-causal-representation-learning)
    - [4.1.3 Causal Discovery](#411-learning-causal-units-and-mechanisms-ie-causal-representation-learning)
    - [4.1.4 Causal Effect Estimation](#414-causal-effect-estimation)
    - [4.1.5 Foundational work (theory, ICA, etc.)](#415-foundational-work-theory-ica-etc)
  - [4.2 Causality Papers from Bengio's Lab, MILA](#42-causality-papers-from-bengios-lab-mila)
    - [Motivational Position Papers](#motivational-position-papers)
    - [Applying Causality Knowledge for RL Interaction Design](#applying-causality-knowledge-for-rl-interaction-design)
    - [Applying causality to model design](#applying-causality-to-model-design)
    - [Causal induction from interventional data](#causal-induction-from-interventional-data)
    - [Grounded AI](#grounded-AI)
  - [4.3 Other Causality Papers (Potentially Applicable to NLP)](#43-other-causality-papers-potentially-applicable-to-nlp)
  - [4.4 Books (for Systematic Learning)](#44-books-for-systematic-learning)
  - [4.5 Online Courses](#45-online-courses)
  - [4.6 People Directory](#46-people-directory)
  - [4.7 Workshops](#47-workshops)
  - [4.8 Others](#48-others)
- [Contributions](#contributions)
- [How to Cite This Repo](#How-to-Cite-This-Repo)
 -->



